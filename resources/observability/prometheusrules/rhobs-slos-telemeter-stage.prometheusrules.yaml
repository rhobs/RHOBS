---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: rhobs-slos-telemeter-stage
spec:
  groups:
  - name: rhobs-telemeter-telemeter-server-metrics-write-availability.slo
    rules:
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsuploadwriteavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(haproxy_server_http_responses_total:burnrate5m{route="telemeter-server-upload"}) > (14.40 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate1h{route="telemeter-server-upload"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        route: telemeter-server-upload
        service: telemeter
        severity: high
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsuploadwriteavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(haproxy_server_http_responses_total:burnrate30m{route="telemeter-server-upload"}) > (6.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate6h{route="telemeter-server-upload"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        route: telemeter-server-upload
        service: telemeter
        severity: high
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsuploadwriteavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(haproxy_server_http_responses_total:burnrate2h{route="telemeter-server-upload"}) > (3.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate1d{route="telemeter-server-upload"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        route: telemeter-server-upload
        service: telemeter
        severity: medium
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsuploadwriteavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(haproxy_server_http_responses_total:burnrate6h{route="telemeter-server-upload"}) > (1.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate3d{route="telemeter-server-upload"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        route: telemeter-server-upload
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$",code="5xx"}[1d]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$"}[1d]))
      labels:
        route: telemeter-server-upload
      record: haproxy_server_http_responses_total:burnrate1d
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$",code="5xx"}[1h]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$"}[1h]))
      labels:
        route: telemeter-server-upload
      record: haproxy_server_http_responses_total:burnrate1h
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$",code="5xx"}[2h]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$"}[2h]))
      labels:
        route: telemeter-server-upload
      record: haproxy_server_http_responses_total:burnrate2h
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$",code="5xx"}[30m]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$"}[30m]))
      labels:
        route: telemeter-server-upload
      record: haproxy_server_http_responses_total:burnrate30m
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$",code="5xx"}[3d]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$"}[3d]))
      labels:
        route: telemeter-server-upload
      record: haproxy_server_http_responses_total:burnrate3d
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$",code="5xx"}[5m]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$"}[5m]))
      labels:
        route: telemeter-server-upload
      record: haproxy_server_http_responses_total:burnrate5m
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$",code="5xx"}[6h]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-upload",code!~"^4..$"}[6h]))
      labels:
        route: telemeter-server-upload
      record: haproxy_server_http_responses_total:burnrate6h
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsreceivewriteavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(haproxy_server_http_responses_total:burnrate5m{route="telemeter-server-metrics-v1-receive"}) > (14.40 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate1h{route="telemeter-server-metrics-v1-receive"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        route: telemeter-server-metrics-v1-receive
        service: telemeter
        severity: high
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsreceivewriteavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(haproxy_server_http_responses_total:burnrate30m{route="telemeter-server-metrics-v1-receive"}) > (6.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate6h{route="telemeter-server-metrics-v1-receive"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        route: telemeter-server-metrics-v1-receive
        service: telemeter
        severity: high
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsreceivewriteavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(haproxy_server_http_responses_total:burnrate2h{route="telemeter-server-metrics-v1-receive"}) > (3.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate1d{route="telemeter-server-metrics-v1-receive"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        route: telemeter-server-metrics-v1-receive
        service: telemeter
        severity: medium
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsreceivewriteavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(haproxy_server_http_responses_total:burnrate6h{route="telemeter-server-metrics-v1-receive"}) > (1.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate3d{route="telemeter-server-metrics-v1-receive"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        route: telemeter-server-metrics-v1-receive
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$",code="5xx"}[1d]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$"}[1d]))
      labels:
        route: telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate1d
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$",code="5xx"}[1h]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$"}[1h]))
      labels:
        route: telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate1h
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$",code="5xx"}[2h]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$"}[2h]))
      labels:
        route: telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate2h
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$",code="5xx"}[30m]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$"}[30m]))
      labels:
        route: telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate30m
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$",code="5xx"}[3d]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$"}[3d]))
      labels:
        route: telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate3d
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$",code="5xx"}[5m]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$"}[5m]))
      labels:
        route: telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate5m
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$",code="5xx"}[6h]))
        /
        sum(rate(haproxy_server_http_responses_total{route="telemeter-server-metrics-v1-receive",code!~"^4..$"}[6h]))
      labels:
        route: telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate6h
  - name: rhobs-telemeter-telemeter-server-metrics-write-latency.slo
    rules:
    - alert: TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=telemeter-server,handler=upload,code!~^4..$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsuploadwritelatencyerrorbudgetburning1h
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1h{job="telemeter-server",handler="upload",latency="5"} > (14.4*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate5m{job="telemeter-server",handler="upload",latency="5"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate6h{job="telemeter-server",handler="upload",latency="5"} > (6*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate30m{job="telemeter-server",handler="upload",latency="5"} > (6*0.100000)
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
        service: telemeter
        severity: high
    - alert: TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=telemeter-server,handler=upload,code!~^4..$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsuploadwritelatencyerrorbudgetburning3d
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1d{job="telemeter-server",handler="upload",latency="5"} > (3*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate2h{job="telemeter-server",handler="upload",latency="5"} > (3*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate3d{job="telemeter-server",handler="upload",latency="5"} > (0.100000)
          and
          latencytarget:http_request_duration_seconds:rate6h{job="telemeter-server",handler="upload",latency="5"} > (0.100000)
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="upload",code!~"^4..$",le="5",code!~"5.."}[5m]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="upload",code!~"^4..$"}[5m]))
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="upload",code!~"^4..$",le="5",code!~"5.."}[30m]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="upload",code!~"^4..$"}[30m]))
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="upload",code!~"^4..$",le="5",code!~"5.."}[1h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="upload",code!~"^4..$"}[1h]))
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="upload",code!~"^4..$",le="5",code!~"5.."}[2h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="upload",code!~"^4..$"}[2h]))
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="upload",code!~"^4..$",le="5",code!~"5.."}[6h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="upload",code!~"^4..$"}[6h]))
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="upload",code!~"^4..$",le="5",code!~"5.."}[1d]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="upload",code!~"^4..$"}[1d]))
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="upload",code!~"^4..$",le="5",code!~"5.."}[3d]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="upload",code!~"^4..$"}[3d]))
        )
      labels:
        handler: upload
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate3d
    - alert: TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=telemeter-server,handler=receive,code!~^4..$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsreceivewritelatencyerrorbudgetburning1h
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1h{job="telemeter-server",handler="receive",latency="5"} > (14.4*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate5m{job="telemeter-server",handler="receive",latency="5"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate6h{job="telemeter-server",handler="receive",latency="5"} > (6*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate30m{job="telemeter-server",handler="receive",latency="5"} > (6*0.100000)
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
        service: telemeter
        severity: high
    - alert: TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-telemeter-server-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=telemeter-server,handler=receive,code!~^4..$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricsreceivewritelatencyerrorbudgetburning3d
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1d{job="telemeter-server",handler="receive",latency="5"} > (3*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate2h{job="telemeter-server",handler="receive",latency="5"} > (3*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate3d{job="telemeter-server",handler="receive",latency="5"} > (0.100000)
          and
          latencytarget:http_request_duration_seconds:rate6h{job="telemeter-server",handler="receive",latency="5"} > (0.100000)
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="receive",code!~"^4..$",le="5",code!~"5.."}[5m]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="receive",code!~"^4..$"}[5m]))
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="receive",code!~"^4..$",le="5",code!~"5.."}[30m]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="receive",code!~"^4..$"}[30m]))
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="receive",code!~"^4..$",le="5",code!~"5.."}[1h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="receive",code!~"^4..$"}[1h]))
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="receive",code!~"^4..$",le="5",code!~"5.."}[2h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="receive",code!~"^4..$"}[2h]))
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="receive",code!~"^4..$",le="5",code!~"5.."}[6h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="receive",code!~"^4..$"}[6h]))
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="receive",code!~"^4..$",le="5",code!~"5.."}[1d]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="receive",code!~"^4..$"}[1d]))
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler="receive",code!~"^4..$",le="5",code!~"5.."}[3d]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler="receive",code!~"^4..$"}[3d]))
        )
      labels:
        handler: receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate3d
  - name: rhobs-telemeter-api-metrics-write-availability.slo
    rules:
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="receive"}) > (14.40 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="receive"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="receive"}) > (6.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="receive"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="receive"}) > (3.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="receive"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="receive"}) > (1.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="receive"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[1d]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[1h]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[2h]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[30m]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[3d]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[5m]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[6h]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
  - name: rhobs-telemeter-api-metrics-write-latency.slo
    rules:
    - alert: APIMetricsWriteLatencyErrorBudgetBurning1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=observatorium-observatorium-api,handler=receive,code!~^4..$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswritelatencyerrorbudgetburning1h
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1h{job="observatorium-observatorium-api",handler="receive",latency="5"} > (14.4*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate5m{job="observatorium-observatorium-api",handler="receive",latency="5"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate6h{job="observatorium-observatorium-api",handler="receive",latency="5"} > (6*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate30m{job="observatorium-observatorium-api",handler="receive",latency="5"} > (6*0.100000)
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
        service: telemeter
        severity: high
    - alert: APIMetricsWriteLatencyErrorBudgetBurning3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=observatorium-observatorium-api,handler=receive,code!~^4..$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswritelatencyerrorbudgetburning3d
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1d{job="observatorium-observatorium-api",handler="receive",latency="5"} > (3*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate2h{job="observatorium-observatorium-api",handler="receive",latency="5"} > (3*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate3d{job="observatorium-observatorium-api",handler="receive",latency="5"} > (0.100000)
          and
          latencytarget:http_request_duration_seconds:rate6h{job="observatorium-observatorium-api",handler="receive",latency="5"} > (0.100000)
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",le="5",code!~"5.."}[5m]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[5m]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",le="5",code!~"5.."}[30m]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[30m]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",le="5",code!~"5.."}[1h]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[1h]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",le="5",code!~"5.."}[2h]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[2h]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",le="5",code!~"5.."}[6h]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[6h]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",le="5",code!~"5.."}[1d]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[1d]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code!~"^4..$",le="5",code!~"5.."}[3d]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code!~"^4..$"}[3d]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate3d
  - name: rhobs-telemeter-api-metrics-read-availability.slo
    rules:
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="query"}) > (14.40 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="query"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="query"}) > (6.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="query"}) > (3.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="query"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query"}) > (1.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="query"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$"}[1d]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$"}[1h]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$"}[2h]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$"}[30m]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$"}[3d]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$"}[5m]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code!~"^4..$"}[6h]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="query_range"}) > (14.40 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="query_range"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="query_range"}) > (6.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query_range"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="query_range"}) > (3.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="query_range"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query_range"}) > (1.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="query_range"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$"}[1d]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$"}[1h]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$"}[2h]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$"}[30m]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$"}[3d]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$"}[5m]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code!~"^4..$"}[6h]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
  - name: rhobs-telemeter-api-metrics-read-latency.slo
    rules:
    - alert: APIMetricsReadLatencyErrorBudgetBurning1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-1M-samples,namespace=observatorium-stage,latency=10 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning1h
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1h{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (14.4*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate5m{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (6*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate30m{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (6*0.100000)
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        service: telemeter
        severity: high
    - alert: APIMetricsReadLatencyErrorBudgetBurning3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-1M-samples,namespace=observatorium-stage,latency=10 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning3d
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1d{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (3*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate2h{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (3*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate3d{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-1M-samples",namespace="observatorium-stage",latency="10"} > (0.100000)
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-stage",le="10",code!~"5.."}[5m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-stage"}[5m]))
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-stage",le="10",code!~"5.."}[30m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-stage"}[30m]))
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-stage",le="10",code!~"5.."}[1h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-stage"}[1h]))
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-stage",le="10",code!~"5.."}[2h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-stage"}[2h]))
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-stage",le="10",code!~"5.."}[6h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-stage"}[6h]))
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-stage",le="10",code!~"5.."}[1d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-stage"}[1d]))
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-stage",le="10",code!~"5.."}[3d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-stage"}[3d]))
        )
      labels:
        latency: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate3d
    - alert: APIMetricsReadLatencyErrorBudgetBurning1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-10M-samples,namespace=observatorium-stage,latency=30 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning1h
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1h{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (14.4*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate5m{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (6*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate30m{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (6*0.100000)
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        service: telemeter
        severity: high
    - alert: APIMetricsReadLatencyErrorBudgetBurning3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-10M-samples,namespace=observatorium-stage,latency=30 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning3d
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1d{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (3*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate2h{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (3*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate3d{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-10M-samples",namespace="observatorium-stage",latency="30"} > (0.100000)
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-stage",le="30",code!~"5.."}[5m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-stage"}[5m]))
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-stage",le="30",code!~"5.."}[30m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-stage"}[30m]))
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-stage",le="30",code!~"5.."}[1h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-stage"}[1h]))
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-stage",le="30",code!~"5.."}[2h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-stage"}[2h]))
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-stage",le="30",code!~"5.."}[6h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-stage"}[6h]))
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-stage",le="30",code!~"5.."}[1d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-stage"}[1d]))
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-stage",le="30",code!~"5.."}[3d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-stage"}[3d]))
        )
      labels:
        latency: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate3d
    - alert: APIMetricsReadLatencyErrorBudgetBurning1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-100M-samples,namespace=observatorium-stage,latency=120 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning1h
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1h{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (14.4*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate5m{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (6*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate30m{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (6*0.100000)
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
        service: telemeter
        severity: high
    - alert: APIMetricsReadLatencyErrorBudgetBurning3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-100M-samples,namespace=observatorium-stage,latency=120 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning3d
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1d{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (3*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate2h{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (3*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate3d{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-100M-samples",namespace="observatorium-stage",latency="120"} > (0.100000)
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-stage",le="120",code!~"5.."}[5m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-stage"}[5m]))
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-stage",le="120",code!~"5.."}[30m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-stage"}[30m]))
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-stage",le="120",code!~"5.."}[1h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-stage"}[1h]))
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-stage",le="120",code!~"5.."}[2h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-stage"}[2h]))
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-stage",le="120",code!~"5.."}[6h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-stage"}[6h]))
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-stage",le="120",code!~"5.."}[1d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-stage"}[1d]))
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-stage",le="120",code!~"5.."}[3d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-stage"}[3d]))
        )
      labels:
        latency: "120"
        namespace: observatorium-stage
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate3d
  - name: rhobs-telemeter-api-rules-raw-write-availability.slo
    rules:
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawwriteavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (14.40 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        service: telemeter
        severity: high
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawwriteavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (6.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        service: telemeter
        severity: high
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawwriteavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (3.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        service: telemeter
        severity: medium
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-write-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawwriteavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (1.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="rules-raw",method="PUT"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$"}[1d]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$"}[1h]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$"}[2h]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$"}[30m]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$"}[3d]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$"}[5m]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",method="PUT",code!~"^4..$"}[6h]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
      record: http_requests_total:burnrate6h
  - name: rhobs-telemeter-api-rules-sync-availability.slo
    rules:
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-sync-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /reload endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulessyncavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(client_api_requests_total:burnrate5m{namespace="observatorium-metrics-stage"}) > (14.40 * (1-0.95000))
        and
        sum(client_api_requests_total:burnrate1h{namespace="observatorium-metrics-stage"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: high
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-sync-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /reload endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulessyncavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(client_api_requests_total:burnrate30m{namespace="observatorium-metrics-stage"}) > (6.00 * (1-0.95000))
        and
        sum(client_api_requests_total:burnrate6h{namespace="observatorium-metrics-stage"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: high
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-sync-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /reload endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulessyncavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(client_api_requests_total:burnrate2h{namespace="observatorium-metrics-stage"}) > (3.00 * (1-0.95000))
        and
        sum(client_api_requests_total:burnrate1d{namespace="observatorium-metrics-stage"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: medium
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-sync-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /reload endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulessyncavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(client_api_requests_total:burnrate6h{namespace="observatorium-metrics-stage"}) > (1.00 * (1-0.95000))
        and
        sum(client_api_requests_total:burnrate3d{namespace="observatorium-metrics-stage"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.+"}[1d]))
        /
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$"}[1d]))
      labels:
        namespace: observatorium-metrics-stage
      record: client_api_requests_total:burnrate1d
    - expr: |
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.+"}[1h]))
        /
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$"}[1h]))
      labels:
        namespace: observatorium-metrics-stage
      record: client_api_requests_total:burnrate1h
    - expr: |
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.+"}[2h]))
        /
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$"}[2h]))
      labels:
        namespace: observatorium-metrics-stage
      record: client_api_requests_total:burnrate2h
    - expr: |
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.+"}[30m]))
        /
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$"}[30m]))
      labels:
        namespace: observatorium-metrics-stage
      record: client_api_requests_total:burnrate30m
    - expr: |
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.+"}[3d]))
        /
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$"}[3d]))
      labels:
        namespace: observatorium-metrics-stage
      record: client_api_requests_total:burnrate3d
    - expr: |
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.+"}[5m]))
        /
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$"}[5m]))
      labels:
        namespace: observatorium-metrics-stage
      record: client_api_requests_total:burnrate5m
    - expr: |
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.+"}[6h]))
        /
        sum(rate(client_api_requests_total{client="reload",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",code!~"^4..$"}[6h]))
      labels:
        namespace: observatorium-metrics-stage
      record: client_api_requests_total:burnrate6h
  - name: rhobs-telemeter-api-rules-read-availability.slo
    rules:
    - alert: APIRulesReadAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesreadavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="rules"}) > (14.40 * (1-0.90000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="rules"}) > (14.40 * (1-0.90000))
      for: 2m
      labels:
        handler: rules
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIRulesReadAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesreadavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="rules"}) > (6.00 * (1-0.90000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="rules"}) > (6.00 * (1-0.90000))
      for: 15m
      labels:
        handler: rules
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIRulesReadAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesreadavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="rules"}) > (3.00 * (1-0.90000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="rules"}) > (3.00 * (1-0.90000))
      for: 1h
      labels:
        handler: rules
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIRulesReadAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesreadavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="rules"}) > (1.00 * (1-0.90000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="rules"}) > (1.00 * (1-0.90000))
      for: 3h
      labels:
        handler: rules
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$"}[1d]))
      labels:
        handler: rules
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$"}[1h]))
      labels:
        handler: rules
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$"}[2h]))
      labels:
        handler: rules
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$"}[30m]))
      labels:
        handler: rules
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$"}[3d]))
      labels:
        handler: rules
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$"}[5m]))
      labels:
        handler: rules
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules",code!~"^4..$"}[6h]))
      labels:
        handler: rules
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
  - name: rhobs-telemeter-api-rules-raw-read-availability.slo
    rules:
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawreadavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="rules-raw"}) > (14.40 * (1-0.90000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="rules-raw"}) > (14.40 * (1-0.90000))
      for: 2m
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawreadavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="rules-raw"}) > (6.00 * (1-0.90000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="rules-raw"}) > (6.00 * (1-0.90000))
      for: 15m
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        service: telemeter
        severity: high
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawreadavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="rules-raw"}) > (3.00 * (1-0.90000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="rules-raw"}) > (3.00 * (1-0.90000))
      for: 1h
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-rules-raw-read-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apirulesrawreadavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="rules-raw"}) > (1.00 * (1-0.90000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="rules-raw"}) > (1.00 * (1-0.90000))
      for: 3h
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$"}[1d]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$"}[1h]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$"}[2h]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$"}[30m]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$"}[3d]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$"}[5m]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",group="metricsv1",handler="rules-raw",code!~"^4..$"}[6h]))
      labels:
        handler: rules-raw
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
  - name: rhobs-telemeter-api-alerting-availability.slo
    rules:
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanageravailabilityerrorbudgetburning5mand1h
      expr: |
        sum(thanos_alert_sender_alerts_dropped_total:burnrate5m{namespace="observatorium-metrics-stage"}) > (14.40 * (1-0.95000))
        and
        sum(thanos_alert_sender_alerts_dropped_total:burnrate1h{namespace="observatorium-metrics-stage"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: high
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanageravailabilityerrorbudgetburning30mand6h
      expr: |
        sum(thanos_alert_sender_alerts_dropped_total:burnrate30m{namespace="observatorium-metrics-stage"}) > (6.00 * (1-0.95000))
        and
        sum(thanos_alert_sender_alerts_dropped_total:burnrate6h{namespace="observatorium-metrics-stage"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: high
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanageravailabilityerrorbudgetburning2hand1d
      expr: |
        sum(thanos_alert_sender_alerts_dropped_total:burnrate2h{namespace="observatorium-metrics-stage"}) > (3.00 * (1-0.95000))
        and
        sum(thanos_alert_sender_alerts_dropped_total:burnrate1d{namespace="observatorium-metrics-stage"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: medium
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanageravailabilityerrorbudgetburning6hand3d
      expr: |
        sum(thanos_alert_sender_alerts_dropped_total:burnrate6h{namespace="observatorium-metrics-stage"}) > (1.00 * (1-0.95000))
        and
        sum(thanos_alert_sender_alerts_dropped_total:burnrate3d{namespace="observatorium-metrics-stage"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[1d]))
        /
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$"}[1d]))
      labels:
        namespace: observatorium-metrics-stage
      record: thanos_alert_sender_alerts_dropped_total:burnrate1d
    - expr: |
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[1h]))
        /
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$"}[1h]))
      labels:
        namespace: observatorium-metrics-stage
      record: thanos_alert_sender_alerts_dropped_total:burnrate1h
    - expr: |
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[2h]))
        /
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$"}[2h]))
      labels:
        namespace: observatorium-metrics-stage
      record: thanos_alert_sender_alerts_dropped_total:burnrate2h
    - expr: |
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[30m]))
        /
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$"}[30m]))
      labels:
        namespace: observatorium-metrics-stage
      record: thanos_alert_sender_alerts_dropped_total:burnrate30m
    - expr: |
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[3d]))
        /
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$"}[3d]))
      labels:
        namespace: observatorium-metrics-stage
      record: thanos_alert_sender_alerts_dropped_total:burnrate3d
    - expr: |
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[5m]))
        /
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$"}[5m]))
      labels:
        namespace: observatorium-metrics-stage
      record: thanos_alert_sender_alerts_dropped_total:burnrate5m
    - expr: |
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[6h]))
        /
        sum(rate(thanos_alert_sender_alerts_dropped_total{container="thanos-rule",namespace="observatorium-metrics-stage",code!~"^4..$"}[6h]))
      labels:
        namespace: observatorium-metrics-stage
      record: thanos_alert_sender_alerts_dropped_total:burnrate6h
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning5mand1h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanagernotificationsavailabilityerrorbudgetburning5mand1h
      expr: |
        sum(alertmanager_notifications_failed_total:burnrate5m{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (14.40 * (1-0.95000))
        and
        sum(alertmanager_notifications_failed_total:burnrate1h{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: high
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning30mand6h
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanagernotificationsavailabilityerrorbudgetburning30mand6h
      expr: |
        sum(alertmanager_notifications_failed_total:burnrate30m{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (6.00 * (1-0.95000))
        and
        sum(alertmanager_notifications_failed_total:burnrate6h{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: high
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning2hand1d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanagernotificationsavailabilityerrorbudgetburning2hand1d
      expr: |
        sum(alertmanager_notifications_failed_total:burnrate2h{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (3.00 * (1-0.95000))
        and
        sum(alertmanager_notifications_failed_total:burnrate1d{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: medium
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning6hand3d
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/rhobs-telemeter-api-alerting-availability.slo?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apialertmanagernotificationsavailabilityerrorbudgetburning6hand3d
      expr: |
        sum(alertmanager_notifications_failed_total:burnrate6h{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (1.00 * (1-0.95000))
        and
        sum(alertmanager_notifications_failed_total:burnrate3d{service="observatorium-alertmanager",namespace="observatorium-metrics-stage"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        namespace: observatorium-metrics-stage
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[1d]))
        /
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$"}[1d]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
      record: alertmanager_notifications_failed_total:burnrate1d
    - expr: |
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[1h]))
        /
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$"}[1h]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
      record: alertmanager_notifications_failed_total:burnrate1h
    - expr: |
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[2h]))
        /
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$"}[2h]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
      record: alertmanager_notifications_failed_total:burnrate2h
    - expr: |
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[30m]))
        /
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$"}[30m]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
      record: alertmanager_notifications_failed_total:burnrate30m
    - expr: |
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[3d]))
        /
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$"}[3d]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
      record: alertmanager_notifications_failed_total:burnrate3d
    - expr: |
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[5m]))
        /
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$"}[5m]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
      record: alertmanager_notifications_failed_total:burnrate5m
    - expr: |
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$",code=~"5.."}[6h]))
        /
        sum(rate(alertmanager_notifications_failed_total{service="observatorium-alertmanager",namespace="observatorium-metrics-stage",code!~"^4..$"}[6h]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
      record: alertmanager_notifications_failed_total:burnrate6h
